{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem definition\n",
    "\n",
    "**States:** For now, I think that it should also be stateless, since all the states I can think of are: \n",
    "- *posture quality:* (poor posture triggers the agent to make an action anyway, so it should be implemented as an iteration itself), \n",
    "- *time of the day:* since the experiment is not long, it can be just an input to the algorithm, for example to have a larger number of iterations\n",
    "- *duration of usage:* could be implemented as $\\epsilon$ the temperature for exploration vs. exploitation...?\n",
    "- *last intervention/outcome:* ?? I don't even know how would this make difference, perhaps in the transition dynamics...\n",
    "- *number of times the experiment has been conducted on that user:* also before the actual experiment???\n",
    "should all of these parameters actually have an impact on those probabilities?\n",
    "BUT, STATELESS IS ALSO A VERY DUMB SOLUTION... in this problem states then wouldn't be dependant on the actions that we take and that is what is confusing to me... \n",
    "\n",
    "Proposal: should the states maybe be EA and EC and actions to transition to the other type of intervention or to remain on the same one?\n",
    "\n",
    "**Actions:** Actions can take value of either EC - error correction or EA - error amplification.\n",
    "\n",
    "**Reward system:** Either -1 (posture not corrected) or +1 (corrected) - how do we collect the actual reward in an iteration, what do we observe?\n",
    "\n",
    "**Event-driven iterations:** Detection in the change of posture from good to poor is an event that triggers the agent to make an action. In the code, that is defined with a loop with max number of iterations. Should it depend on the user though? For example, we have some general information collected about the user, whether he/she has bad posture generally, what time of the day it is (if it's evening, the subject is most likely tired and has a bad posture - so interventions are triggered much more frequently, leading to more iterations overall).\n",
    "\n",
    "**Non-episodic learning:** We start from Q values set to zero each time, this means that we don't learn anything from experiment to experiment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Keep using keras-2 (tf-keras) rather than keras-3 (keras).\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tf_agents.environments as tf_env\n",
    "import tf_agents.specs as tf_specs\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.trajectories import time_step as ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For now this is a stateless implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningEnv(tf_env.py_environment.PyEnvironment):\n",
    "    # Encode action:\n",
    "    # EC = 0\n",
    "    # EA = 1\n",
    "    def __init__(self, user_model_vec):\n",
    "        # Defining the action space\n",
    "        self._num_actions = 2\n",
    "        # User specifications (intervention preferences) - probabilities vector\n",
    "        self._user_model_vec = user_model_vec\n",
    "\n",
    "        # Initialization of Q values - TO MODIFY IF WE HAVE MORE STATES...\n",
    "        self.Q = np.array([0.0, 0.0], dtype=np.float32)\n",
    "        #self._true_action_values = 2 * np.array(user_model_vec, dtype=np.float32) - 1\n",
    "        self._step_count = 0\n",
    "        self._total_reward = 0.0\n",
    "        super(QLearningEnv, self).__init__()\n",
    "\n",
    "    def action_spec(self):\n",
    "        return tf_specs.BoundedArraySpec(\n",
    "            shape=(), dtype=np.int32, minimum=0, maximum=self._num_actions - 1, name='action')\n",
    "\n",
    "    def observation_spec(self):\n",
    "        return tf_specs.BoundedArraySpec(\n",
    "            shape=(self._num_actions,), dtype=np.float32, minimum=0, name='observation')\n",
    "\n",
    "    def _reset(self):\n",
    "        self.Q = np.array([0.0, 0.0], dtype=np.float32) #non-episodic learning\n",
    "        self._step_count = 0\n",
    "        self._total_reward = 0.0\n",
    "        return ts.restart(observation = None)\n",
    "\n",
    "    def _step(self, action, alpha):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        action - chosen action of the step with eps-greedy exploration\n",
    "        alpha - a.k.a. learning_rate\n",
    "        \"\"\"\n",
    "        # TODO: integrate with the physical environment here\n",
    "\n",
    "        # define the reward method\n",
    "        # reward = np.random.normal(self._true_action_values[action], 1.0) # the gaussian reward\n",
    "        reward = 1 if np.random.uniform(0,1) < self._user_model_vec[action] else -1\n",
    "\n",
    "        self._step_count += 1\n",
    "        self._total_reward += reward\n",
    "\n",
    "        # Updating Q value of chosen action\n",
    "        self.Q[action] = alpha*reward + (1-alpha)*self.Q[action] # it is a lot simpler because we have no states \n",
    "\n",
    "        if self._step_count == 1000:  # Limit the number of steps for this example - do we need this???\n",
    "            return ts.termination(observation = None, reward = reward)\n",
    "        else:\n",
    "            return ts.transition(observation=None, reward = reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsGreedyAgent:\n",
    "    def __init__(self, num_actions, explore_rate = 1.0): #JEL JE OVO OK??? \n",
    "        self.num_actions = num_actions\n",
    "        self.action_values = [0.0] * num_actions\n",
    "        self.action_counts = [0] * num_actions\n",
    "        self.step_counts = 0\n",
    "        self.explore = explore_rate  \n",
    "        self.reward_history = []\n",
    "\n",
    "    def select_action(self, env:QLearningEnv):\n",
    "        # Should be selected on the basis of epsilon greedy\n",
    "\n",
    "        explore = np.random.binomial(2, p=self.explore)\n",
    "        if explore:\n",
    "            # Exploration: With probability epsilon take a random action, an index of an action\n",
    "            a = np.random.choice(np.arange(self.num_actions))\n",
    "        else:\n",
    "            # Exploitation: With probability 1 - epsilon take one of the optimal actions for the current state\n",
    "            a = np.argmax(env.Q)\n",
    "\n",
    "        return a\n",
    "\n",
    "    def update_estimates(self, action, reward):\n",
    "        \n",
    "        \"\"\"\n",
    "        Updates the values of action counts, step counts, reward history etc.\n",
    "\n",
    "        Inputs:\n",
    "        action - action taken in the current step\n",
    "        reward - reward collected while taking the step when interacting with the environment\n",
    "\n",
    "        \"\"\"\n",
    "        #??? sta ovde treba da stavim ???\n",
    "        self.action_counts[action] += 1\n",
    "        self.step_counts += 1\n",
    "        self.reward_history.append(reward)\n",
    "\n",
    "\n",
    "    def get_current_average_reward(self):\n",
    "        return np.mean(self.reward_history)\n",
    "    \n",
    "    def get_cumulative_reward(self):\n",
    "        return np.sum(self.reward_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_simulation(user_model_vec, num_iterations = 500, explore_rate = 2.0, alpha = 0.5):\n",
    "  \"\"\"\n",
    "  Inputs:\n",
    "\n",
    "  user_model_vec:\n",
    "  num_iterations: maximal number of iterations, in our problem this refers to the number of times the posture becomes poor, \n",
    "  i.e. number of interventions - should be modified to be dependant on the user and its attributes...\n",
    "  explore_rate: maybe modify to be dependant on the period of the day...? explore more if it is evening and more fatigue...?\n",
    "\n",
    "  Outputs:\n",
    "\n",
    "  average_rewards:\n",
    "  rmse:\n",
    "  reward_history:\n",
    "\n",
    "  \"\"\"\n",
    "  # Create the environment\n",
    "  \n",
    "  #true_action_value = 2 * np.array(user_model_vec) - 1\n",
    "  env = QLearningEnv(user_model_vec)\n",
    "\n",
    "  # Create the epsilon greedy agent\n",
    "  agent = EpsGreedyAgent(num_actions=env.action_spec().maximum + 1, explore_rate=explore_rate)\n",
    "\n",
    "  average_rewards = []\n",
    "  rmse = []\n",
    "\n",
    "  # Training loop\n",
    "  for _ in range(num_iterations):\n",
    "      # Take one step \n",
    "      action = agent.select_action(env)\n",
    "      time_step = env._step(action, alpha)\n",
    "      agent.update_estimates(action, time_step.reward)\n",
    "\n",
    "      # Logs for plotting\n",
    "      average_rewards.append(agent.get_current_average_reward()) # should we plot current average reward or current reward that is collected??\n",
    "      \n",
    "\n",
    "  # Evaluate the agent\n",
    "  if False:\n",
    "    total_reward = 0.0\n",
    "    for _ in range(10):  # Test for 10 episodes\n",
    "        time_step = env.reset()\n",
    "        while not time_step.is_last():\n",
    "            action = agent.select_action()\n",
    "            time_step = env.step(action)\n",
    "            total_reward += time_step.reward\n",
    "    print(f'Average Reward of 10 episodes: {total_reward / 10 / 1000}')\n",
    "\n",
    "  return average_rewards, rmse, agent.reward_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation on one user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "average_rewards, rmse, reward_history = user_simulation(user_model_vec = [0.1, 0.9], explore_rate=0.1, alpha=0.7)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(average_rewards)\n",
    "plt.ylabel('Average Reward')\n",
    "plt.xlabel('Number of Iterations')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcement_ass1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
